{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cf647b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ghd92\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: regex in c:\\users\\ghd92\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ghd92\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\ghd92\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\ghd92\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d9921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "newsgroups_test = fetch_20newsgroups(subset='test')\n",
    "\n",
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print (\"20개 카테고리 전체 목록:\")\n",
    "print (newsgroups_train.target_names)\n",
    "print (\"\\n\")\n",
    "print (\"샘플 이메일:\")\n",
    "print (x_train[0])\n",
    "print (\"샘플 타깃 카테고리:\")\n",
    "print (y_train[0])\n",
    "print (newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febf18f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리에 사용\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2169e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in\n",
    "              nltk.word_tokenize(sent)]\n",
    "    \n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    \n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "        \n",
    "    tagged_corpus = pos_tag(tokens)    \n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def prat_lemmatize(token,tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "    \n",
    "    pre_proc_text =  \" \".join([prat_lemmatize(token,tag) for token,tag in tagged_corpus])             \n",
    "\n",
    "    return pre_proc_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fbab02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed  = []\n",
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c58bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF 벡터라이저(vectorizer) 구축 \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0511a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 딥러닝 모듈\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5025766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 정의\n",
    "np.random.seed(1337) \n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372be6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스에서의 딥 레이어(심층) 모델 구축\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1000,input_shape= (10000,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0246b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bae74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass = np.argmax(model.predict(x_train_2_),axis = 1)\n",
    "y_test_predclass = np.argmax(model.predict(x_test_2),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb7df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d8f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e88d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8de378",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6b79d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "print (\"\\n\\n딥 뉴럴 네트워크  - 학습 정확도:\"),(round(accuracy_score(y_train,y_train_predclass),3))\n",
    "print (\"\\n딥 뉴럴 네트워크  - 테스트 정확도:\"),(round(accuracy_score(y_test,y_test_predclass),3))\n",
    "\n",
    "print (\"\\n딥 뉴럴 네트워크  - 학습 분류 리포트(Train Classification Report)\")\n",
    "print (classification_report(y_train,y_train_predclass))\n",
    "\n",
    "print (\"\\n딥 뉴럴 네트워크  - 테스트 분류 리포트(Test Classification Report)\")\n",
    "print (classification_report(y_test,y_test_predclass))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3cbe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
